{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe7623d8",
   "metadata": {},
   "source": [
    "**Problem Summary**\n",
    "\n",
    "To replicate the Deep Structural Causal Models for Tractable Counterfactual Inference[1]paper , and apply it to google cartoon faces dataset[3] and answer counterfactual queries on the same. \n",
    "\n",
    "We aim to explicitly model causal relationships with a fully specified causal models with no unobserved confounding and inferring exogenous noise via  normalising flows.\n",
    "\n",
    "Our goal is to validate our causal assumptions; if our causal assumptions are valid, these simulations should align with our imagination. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500a9e40",
   "metadata": {},
   "source": [
    "**Data Generation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63f94d0",
   "metadata": {},
   "source": [
    "We use the google cartoon dataset to train our model. The original 4D (10 k) dataset was transformed into lower dimension grayscale dataset due to computation resource limitation. \n",
    "\n",
    "Also, for simplicity we convert the categorical feature into binary (glasses/no glasses)\n",
    "\n",
    "![Dataset Snapshot](./images/cartoon_snapshot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba35ca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path_1,path_2):\n",
    "    \"\"\"Open the folder for cartoon dataset and combine them into one dataset with added column flenames which stores\n",
    "    the corresponding png filename\n",
    "    :param path_1: File path to the cartoon dataset directory\n",
    "    :param path_2: File path to save the cobined csv file\n",
    "    :returns: saves the combined file to the given path_2\"\"\"\n",
    "\n",
    "    import os\n",
    "    import glob\n",
    "    import pandas as pd\n",
    "\n",
    "    os.chdir(path_1)\n",
    "    #os.chdir(\"Documents/GitHub/causalfairness/cartoonset10k/\")\n",
    "    extension = 'csv'\n",
    "    all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "\n",
    "    # combine all files in the list\n",
    "    combined_csv = pd.DataFrame()\n",
    "    for f in all_filenames:\n",
    "        df = pd.read_csv(f)\n",
    "        df = df.T\n",
    "        df = df.reset_index()\n",
    "        df = df.drop([0,2])\n",
    "        df['filename'] = f\n",
    "        df = df.reset_index()\n",
    "        combined_csv = combined_csv.append(df,ignore_index=True)\n",
    "\n",
    "    # export to csv\n",
    "    combined_csv = combined_csv.drop(columns=['level_0'])\n",
    "    combined_csv.columns = ['eye_angle','eye_lashes','eye_lid','chin_length','eyebrow_weight','eyebrow_shape','eyebrow_thickness','face_shape','facial_hair','hair','eye_color','face_color','hair_color','glasses','glasses_color','eye_slant','eyebrow_width','eye_eyebrow_distance','filename']\n",
    "    combined_csv.to_csv(path_2+\"combined_csv.csv\", index=False, encoding='utf-8-sig')\n",
    "    return\n",
    "\n",
    "def columntobinary(path_1,path_2):\n",
    "    \"\"\"Open the combined data file and make a filtered binary datafile\n",
    "    :param path_1: File path to the combined file\n",
    "    :param path_2: File path to save binary file\n",
    "    :returns: saves the binary file to path_2\"\"\"\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(path_1)\n",
    "    df[\"facial_hair\"].replace({14: 0}, inplace=True)\n",
    "    df[\"glasses\"].replace({11: 0}, inplace=True)\n",
    "    df.to_csv(path_2+\"filtered_data_binary_new.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(df)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d4958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "CARTOON_PATH = \"/content/drive/MyDrive/data/cartoon-faces/\"\n",
    "cartoon_features_images_fname = \"cartoon_features_filenames.csv\"\n",
    "\n",
    "cartoon_features_images = pd.read_csv(CARTOON_PATH+cartoon_features_images_fname)\n",
    "cartoon_features_images = cartoon_features_images[[\"face_shape\", \"glasses\", \"filename\"]]\n",
    "\n",
    "cartoon_features_images.glasses.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28fe327",
   "metadata": {},
   "source": [
    "**Model Pieces Explained**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f8bb82",
   "metadata": {},
   "source": [
    "**Normalizing Flows**\n",
    "\n",
    "\n",
    "Normalizing flows are a family of methods which allows for constructing more flexible probability distributions, commonly learned using neural networks. \n",
    "\n",
    "The path traversed by the random variables is the flow and the full chain formed by the successive distributions   is called a normalizing flow. \n",
    "\n",
    "Required by the computation in the equation, a transformation function  should satisfy two properties:\n",
    "\n",
    "    1. It is easily invertible.\n",
    "    2. Its Jacobian determinant is easy to compute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921b948b",
   "metadata": {},
   "source": [
    "**Normalizing Flows on Images**\n",
    "\n",
    "Build a normalizing flow that maps an input image  to an equally sized latent space. \n",
    "\n",
    "Training and Validation: Perform density estimation in the forward direction by applying  a series of flow transformations on the input 𝑥 and estimate the probability of the input by determining the probability of the transformed point  𝑧  given a prior, and the change of volume caused by the transformations. \n",
    "\n",
    "Inference:  density estimation and sample new points by inverting the flow transformations\n",
    "\n",
    "\n",
    "![Normalizing flows on Images](./images/nf_images.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b4773e",
   "metadata": {},
   "source": [
    "**Coupling Layers**\n",
    "\n",
    "A popular flow layer which lends itself to the architecture of neural networks is the coupling layer\n",
    "\n",
    "A given input z is split into two parts, the first part is passed through unchanged while the second part has a function dependent on both parts applied to it\n",
    "\n",
    "A standard version is the affine transformation, implemented as: $z_{1:j}'' = z_{1:j}, z_{j+1,d}' = \\mu(z_{1:j}) + \\sigma(z_{1:j})*z_{j+1:d}$ \n",
    "\n",
    "![Coupling Layers](./images/coupling_layers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ff59fb",
   "metadata": {},
   "source": [
    "**Auxillary Variables Model Pieces**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f605a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions.utils import lazy_property\n",
    "from torch.distributions import constraints\n",
    "from torch.distributions.transforms import Transform\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SqueezeTransform(Transform):\n",
    "    \"\"\"A transformation defined for image data that trades spatial dimensions for channel\n",
    "    dimensions, i.e. \"squeezes\" the inputs along the channel dimensions.\n",
    "    Implementation adapted from https://github.com/pclucas14/pytorch-glow and\n",
    "    https://github.com/chaiyujin/glow-pytorch.\n",
    "    Reference:\n",
    "    > L. Dinh et al., Density estimation using Real NVP, ICLR 2017.\n",
    "    \"\"\"\n",
    "\n",
    "    codomain = constraints.real\n",
    "    bijective = True\n",
    "    event_dim = 3\n",
    "    volume_preserving = True\n",
    "\n",
    "    def __init__(self, factor=2):\n",
    "        super().__init__(cache_size=1)\n",
    "\n",
    "        self.factor = factor\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        \"\"\"\n",
    "        :param x: the input into the bijection\n",
    "        :type x: torch.Tensor\n",
    "        Invokes the bijection x=>y; in the prototypical context of a\n",
    "        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from\n",
    "        the base distribution (or the output of a previous transform)\n",
    "        \"\"\"\n",
    "        if inputs.dim() < 3:\n",
    "            raise ValueError(f'Expecting inputs with at least 3 dimensions, got {inputs.shape} - {inputs.dim()}')\n",
    "\n",
    "        *batch_dims, c, h, w = inputs.size()\n",
    "        num_batch = len(batch_dims)\n",
    "\n",
    "        if h % self.factor != 0 or w % self.factor != 0:\n",
    "            raise ValueError('Input image size not compatible with the factor.')\n",
    "\n",
    "        inputs = inputs.view(*batch_dims, c, h // self.factor, self.factor, w // self.factor,\n",
    "                             self.factor)\n",
    "        permute = np.array((0, 2, 4, 1, 3)) + num_batch\n",
    "        inputs = inputs.permute(*np.arange(num_batch), *permute).contiguous()\n",
    "        inputs = inputs.view(*batch_dims, c * self.factor * self.factor, h // self.factor,\n",
    "                             w // self.factor)\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def _inverse(self, inputs):\n",
    "        \"\"\"\n",
    "        :param y: the output of the bijection\n",
    "        :type y: torch.Tensor\n",
    "        Inverts y => x.\n",
    "        \"\"\"\n",
    "        if inputs.dim() < 3:\n",
    "            raise ValueError(f'Expecting inputs with at least 3 dimensions, got {inputs.shape}')\n",
    "\n",
    "        *batch_dims, c, h, w = inputs.size()\n",
    "        num_batch = len(batch_dims)\n",
    "\n",
    "        if c < 4 or c % 4 != 0:\n",
    "            raise ValueError('Invalid number of channel dimensions.')\n",
    "\n",
    "        inputs = inputs.view(*batch_dims, c // self.factor ** 2, self.factor, self.factor, h, w)\n",
    "        permute = np.array((0, 3, 1, 4, 2)) + num_batch\n",
    "        inputs = inputs.permute(*np.arange(num_batch), *permute).contiguous()\n",
    "        inputs = inputs.view(*batch_dims, c // self.factor ** 2, h * self.factor, w * self.factor)\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def log_abs_det_jacobian(self, x, y):\n",
    "        \"\"\"\n",
    "        Calculates the elementwise determinant of the log Jacobian, i.e.\n",
    "        log(abs([dy_0/dx_0, ..., dy_{N-1}/dx_{N-1}])). Note that this type of\n",
    "        transform is not autoregressive, so the log Jacobian is not the sum of the\n",
    "        previous expression. However, it turns out it's always 0 (since the\n",
    "        determinant is -1 or +1), and so returning a vector of zeros works.\n",
    "        \"\"\"\n",
    "\n",
    "        log_abs_det_jacobian = torch.zeros(x.size()[:-3], dtype=x.dtype, layout=x.layout, device=x.device)\n",
    "        return log_abs_det_jacobian\n",
    "\n",
    "    def get_output_shape(self, c, h, w):\n",
    "        return (c * self.factor * self.factor,\n",
    "                h // self.factor,\n",
    "                w // self.factor)\n",
    "\n",
    "\n",
    "class ReshapeTransform(Transform):\n",
    "    codomain = constraints.real\n",
    "    bijective = True\n",
    "    volume_preserving = True\n",
    "\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        super().__init__()\n",
    "        self.event_dim = len(input_shape)\n",
    "        self.inv_event_dim = len(output_shape)\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        \"\"\"\n",
    "        :param x: the input into the bijection\n",
    "        :type x: torch.Tensor\n",
    "        Invokes the bijection x=>y; in the prototypical context of a\n",
    "        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from\n",
    "        the base distribution (or the output of a previous transform)\n",
    "        \"\"\"\n",
    "        batch_dims = inputs.shape[:-self.event_dim]\n",
    "        inp_shape = inputs.shape[-self.event_dim:]\n",
    "        if inp_shape != self.input_shape:\n",
    "            raise RuntimeError('Unexpected inputs shape ({}, but expecting {})'\n",
    "                               .format(inp_shape, self.input_shape))\n",
    "        return inputs.reshape(*batch_dims, *self.output_shape)\n",
    "\n",
    "    def _inverse(self, inputs):\n",
    "        \"\"\"\n",
    "        :param y: the output of the bijection\n",
    "        :type y: torch.Tensor\n",
    "        Inverts y => x.\n",
    "        \"\"\"\n",
    "        batch_dims = inputs.shape[:-self.inv_event_dim]\n",
    "        inp_shape = inputs.shape[-self.inv_event_dim:]\n",
    "        if inp_shape != self.output_shape:\n",
    "            raise RuntimeError('Unexpected inputs shape ({}, but expecting {})'\n",
    "                               .format(inp_shape, self.output_shape))\n",
    "        return inputs.reshape(*batch_dims, *self.input_shape)\n",
    "\n",
    "    def log_abs_det_jacobian(self, x, y):\n",
    "        \"\"\"\n",
    "        Calculates the elementwise determinant of the log Jacobian, i.e.\n",
    "        log(abs([dy_0/dx_0, ..., dy_{N-1}/dx_{N-1}])). Note that this type of\n",
    "        transform is not autoregressive, so the log Jacobian is not the sum of the\n",
    "        previous expression. However, it turns out it's always 0 (since the\n",
    "        determinant is -1 or +1), and so returning a vector of zeros works.\n",
    "        \"\"\"\n",
    "\n",
    "        log_abs_det_jacobian = torch.zeros(x.size()[:-self.event_dim], dtype=x.dtype, layout=x.layout, device=x.device)\n",
    "        return log_abs_det_jacobian\n",
    "\n",
    "\n",
    "class TransposeTransform(Transform):\n",
    "    \"\"\"\n",
    "    A bijection that reorders the input dimensions, that is, multiplies the input by\n",
    "    a permutation matrix. This is useful in between\n",
    "    :class:`~pyro.distributions.transforms.AffineAutoregressive` transforms to\n",
    "    increase the flexibility of the resulting distribution and stabilize learning.\n",
    "    Whilst not being an autoregressive transform, the log absolute determinate of\n",
    "    the Jacobian is easily calculable as 0. Note that reordering the input dimension\n",
    "    between two layers of\n",
    "    :class:`~pyro.distributions.transforms.AffineAutoregressive` is not equivalent\n",
    "    to reordering the dimension inside the MADE networks that those IAFs use; using\n",
    "    a :class:`~pyro.distributions.transforms.Permute` transform results in a\n",
    "    distribution with more flexibility.\n",
    "    Example usage:\n",
    "    >>> from pyro.nn import AutoRegressiveNN\n",
    "    >>> from pyro.distributions.transforms import AffineAutoregressive, Permute\n",
    "    >>> base_dist = dist.Normal(torch.zeros(10), torch.ones(10))\n",
    "    >>> iaf1 = AffineAutoregressive(AutoRegressiveNN(10, [40]))\n",
    "    >>> ff = Permute(torch.randperm(10, dtype=torch.long))\n",
    "    >>> iaf2 = AffineAutoregressive(AutoRegressiveNN(10, [40]))\n",
    "    >>> flow_dist = dist.TransformedDistribution(base_dist, [iaf1, ff, iaf2])\n",
    "    >>> flow_dist.sample()  # doctest: +SKIP\n",
    "    :param permutation: a permutation ordering that is applied to the inputs.\n",
    "    :type permutation: torch.LongTensor\n",
    "    \"\"\"\n",
    "\n",
    "    codomain = constraints.real\n",
    "    bijective = True\n",
    "    volume_preserving = True\n",
    "\n",
    "    def __init__(self, permutation):\n",
    "        super().__init__(cache_size=1)\n",
    "\n",
    "        self.event_dim = len(permutation)\n",
    "        self.permutation = permutation\n",
    "\n",
    "    @lazy_property\n",
    "    def inv_permutation(self):\n",
    "        result = torch.empty_like(self.permutation, dtype=torch.long)\n",
    "        result[self.permutation] = torch.arange(self.permutation.size(0),\n",
    "                                                dtype=torch.long,\n",
    "                                                device=self.permutation.device)\n",
    "        return result\n",
    "\n",
    "    def _call(self, x):\n",
    "        \"\"\"\n",
    "        :param x: the input into the bijection\n",
    "        :type x: torch.Tensor\n",
    "        Invokes the bijection x=>y; in the prototypical context of a\n",
    "        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from\n",
    "        the base distribution (or the output of a previous transform)\n",
    "        \"\"\"\n",
    "\n",
    "        *batch_dims, c, h, w = x.size()\n",
    "        num_batch = len(batch_dims)\n",
    "\n",
    "        return x.permute(*np.arange(num_batch), *(self.permutation + num_batch)).contiguous()\n",
    "\n",
    "    def _inverse(self, y):\n",
    "        \"\"\"\n",
    "        :param y: the output of the bijection\n",
    "        :type y: torch.Tensor\n",
    "        Inverts y => x.\n",
    "        \"\"\"\n",
    "\n",
    "        *batch_dims, c, h, w = y.size()\n",
    "        num_batch = len(batch_dims)\n",
    "\n",
    "        return y.permute(*np.arange(num_batch), *(self.inv_permutation + num_batch)).contiguous()\n",
    "\n",
    "    def log_abs_det_jacobian(self, x, y):\n",
    "        \"\"\"\n",
    "        Calculates the elementwise determinant of the log Jacobian, i.e.\n",
    "        log(abs([dy_0/dx_0, ..., dy_{N-1}/dx_{N-1}])). Note that this type of\n",
    "        transform is not autoregressive, so the log Jacobian is not the sum of the\n",
    "        previous expression. However, it turns out it's always 0 (since the\n",
    "        determinant is -1 or +1), and so returning a vector of zeros works.\n",
    "        \"\"\"\n",
    "\n",
    "        log_abs_det_jacobian = torch.zeros(x.size()[:-self.event_dim], dtype=x.dtype, layout=x.layout, device=x.device)\n",
    "        return log_abs_det_jacobian\n",
    "\n",
    "\n",
    "from pyro.distributions.conditional import ConditionalTransformModule\n",
    "from pyro.distributions.torch_transform import TransformModule\n",
    "from pyro.distributions import transforms as pyro_transforms\n",
    "from torch.distributions import transforms\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class LearnedAffineTransform(TransformModule, transforms.AffineTransform):\n",
    "    def __init__(self, loc=None, scale=None, **kwargs):\n",
    "\n",
    "        super().__init__(loc=loc, scale=scale, **kwargs)\n",
    "\n",
    "        if loc is None:\n",
    "            self.loc = torch.nn.Parameter(torch.zeros([1, ]))\n",
    "        if scale is None:\n",
    "            self.scale = torch.nn.Parameter(torch.ones([1, ]))\n",
    "\n",
    "    def _broadcast(self, val):\n",
    "        dim_extension = tuple(1 for _ in range(val.dim() - 1))\n",
    "        loc = self.loc.view(-1, *dim_extension)\n",
    "        scale = self.scale.view(-1, *dim_extension)\n",
    "\n",
    "        return loc, scale\n",
    "\n",
    "    def _call(self, x):\n",
    "        loc, scale = self._broadcast(x)\n",
    "\n",
    "        return loc + scale * x\n",
    "\n",
    "    def _inverse(self, y):\n",
    "        loc, scale = self._broadcast(y)\n",
    "        return (y - loc) / scale\n",
    "\n",
    "\n",
    "class ConditionalAffineTransform(ConditionalTransformModule):\n",
    "    def __init__(self, context_nn, event_dim=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.event_dim = event_dim\n",
    "        self.context_nn = context_nn\n",
    "\n",
    "    def condition(self, context):\n",
    "        loc, log_scale = self.context_nn(context)\n",
    "        scale = torch.exp(log_scale)\n",
    "\n",
    "        ac = transforms.AffineTransform(loc, scale, event_dim=self.event_dim)\n",
    "        return ac\n",
    "\n",
    "\n",
    "class LowerCholeskyAffine(pyro_transforms.LowerCholeskyAffine):\n",
    "    def log_abs_det_jacobian(self, x, y):\n",
    "        \"\"\"\n",
    "        Calculates the elementwise determinant of the log Jacobian, i.e.\n",
    "        log(abs(dy/dx)).\n",
    "        \"\"\"\n",
    "        return torch.ones(x.size()[:-1], dtype=x.dtype, layout=x.layout, device=x.device) * \\\n",
    "            self.scale_tril.diagonal(dim1=-2, dim2=-1).log().sum(-1).sum(-1)\n",
    "\n",
    "\n",
    "class ActNorm(TransformModule):\n",
    "    codomain = constraints.real\n",
    "    bijective = True\n",
    "    event_dim = 3\n",
    "\n",
    "    def __init__(self, features):\n",
    "        \"\"\"\n",
    "        Transform that performs activation normalization. Works for 2D and 4D inputs. For 4D\n",
    "        inputs (images) normalization is performed per-channel, assuming BxCxHxW input shape.\n",
    "        Reference:\n",
    "        > D. Kingma et. al., Glow: Generative flow with invertible 1x1 convolutions, NeurIPS 2018.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.initialized = False\n",
    "        self.log_scale = nn.Parameter(torch.zeros(features))\n",
    "        self.shift = nn.Parameter(torch.zeros(features))\n",
    "\n",
    "    @property\n",
    "    def scale(self):\n",
    "        return torch.exp(self.log_scale)\n",
    "\n",
    "    def _broadcastable_scale_shift(self, inputs):\n",
    "        if inputs.dim() == 4:\n",
    "            return self.scale.view(1, -1, 1, 1), self.shift.view(1, -1, 1, 1)\n",
    "        else:\n",
    "            return self.scale.view(1, -1), self.shift.view(1, -1)\n",
    "\n",
    "    def _call(self, x):\n",
    "        if x.dim() not in [2, 4]:\n",
    "            raise ValueError(\"Expecting inputs to be a 2D or a 4D tensor.\")\n",
    "\n",
    "        if self.training and not self.initialized:\n",
    "            self._initialize(x)\n",
    "\n",
    "        scale, shift = self._broadcastable_scale_shift(x)\n",
    "        outputs = scale * x + shift\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _inverse(self, y):\n",
    "        if y.dim() not in [2, 4]:\n",
    "            raise ValueError(\"Expecting inputs to be a 2D or a 4D tensor.\")\n",
    "\n",
    "        scale, shift = self._broadcastable_scale_shift(y)\n",
    "        outputs = (y - shift) / scale\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def log_abs_det_jacobian(self, x, y):\n",
    "        \"\"\"\n",
    "        Calculates the elementwise determinant of the log Jacobian, i.e.\n",
    "        log(abs([dy_0/dx_0, ..., dy_{N-1}/dx_{N-1}])). Note that this type of\n",
    "        transform is not autoregressive, so the log Jacobian is not the sum of the\n",
    "        previous expression. However, it turns out it's always 0 (since the\n",
    "        determinant is -1 or +1), and so returning a vector of zeros works.\n",
    "        \"\"\"\n",
    "\n",
    "        ones = torch.ones(x.shape[0], device=x.device)\n",
    "        if x.dim() == 4:\n",
    "            _, _, h, w = x.shape\n",
    "            log_abs_det_jacobian = h * w * torch.sum(self.log_scale) * ones\n",
    "        else:\n",
    "            log_abs_det_jacobian = torch.sum(self.log_scale) * ones\n",
    "\n",
    "        return log_abs_det_jacobian\n",
    "\n",
    "    def _initialize(self, inputs):\n",
    "        \"\"\"Data-dependent initialization, s.t. post-actnorm activations have zero mean and unit\n",
    "        variance. \"\"\"\n",
    "        if inputs.dim() == 4:\n",
    "            num_channels = inputs.shape[1]\n",
    "            inputs = inputs.permute(0, 2, 3, 1).reshape(-1, num_channels)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            std = inputs.std(dim=0)\n",
    "            mu = (inputs / std).mean(dim=0)\n",
    "            self.log_scale.data = -torch.log(std)\n",
    "            self.shift.data = -mu\n",
    "\n",
    "        self.initialized = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a06f89",
   "metadata": {},
   "source": [
    "**Image Model Pieces**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c85a88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "class BasicFlowConvNet(nn.Module):\n",
    "    def __init__(self, in_channels: int, hidden_channels: int, param_dims, context_dims: int = None, param_nonlinearities=None):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "        self.param_dims = param_dims\n",
    "        self.count_params = len(param_dims)\n",
    "        self.output_dims = sum(param_dims)\n",
    "\n",
    "        self.context_dims = context_dims\n",
    "        self.param_nonlinearities = param_nonlinearities\n",
    "\n",
    "        self.seq1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels + context_dims if context_dims is not None else in_channels, hidden_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_channels, self.output_dims, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        ends = torch.cumsum(torch.tensor(param_dims), dim=0)\n",
    "        starts = torch.cat((torch.zeros(1).type_as(ends), ends[:-1]))\n",
    "        self.param_slices = [slice(s.item(), e.item()) for s, e in zip(starts, ends)]\n",
    "\n",
    "        def weights_init(m):\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight.data, 0., 1e-4)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias.data, 0.)\n",
    "\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, inputs, context=None):\n",
    "        # pyro affine coupling splits on the last dimenion and not the channel dimension\n",
    "        # -> we want to permute the dimensions to move the last dimension into the channel dimension\n",
    "        # and then transpose back\n",
    "\n",
    "        if not ((self.context_dims is None) == (context is None)):\n",
    "            raise ValueError('Given context does not match context dims: context: {} and context_dims:{}'.format(context, self.context_dims))\n",
    "\n",
    "        *batch_dims, h, w, c = inputs.size()\n",
    "        num_batch = len(batch_dims)\n",
    "\n",
    "        permutation = np.array((2, 0, 1)) + num_batch\n",
    "        outputs = inputs.permute(*np.arange(num_batch), *permutation).contiguous()\n",
    "\n",
    "        if context is not None:\n",
    "            # assuming scalar inputs [B, C]\n",
    "            context = context.view(*context.shape, 1, 1).expand(-1, -1, *outputs.shape[2:])\n",
    "            outputs = torch.cat([outputs, context], 1)\n",
    "\n",
    "        outputs = self.seq1(outputs)\n",
    "\n",
    "        permutation = np.array((1, 2, 0)) + num_batch\n",
    "        outputs = outputs.permute(*np.arange(num_batch), *permutation).contiguous()\n",
    "\n",
    "        if self.count_params > 1:\n",
    "            outputs = tuple(outputs[..., s] for s in self.param_slices)\n",
    "\n",
    "        if self.param_nonlinearities is not None:\n",
    "            if isinstance(self.param_nonlinearities, Iterable):\n",
    "                outputs = tuple(n(o) for o, n in zip(outputs, self.param_nonlinearities))\n",
    "            else:\n",
    "                outputs = tuple(self.param_nonlinearities(o) for o in outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5beb1c",
   "metadata": {},
   "source": [
    "**Full Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c5f7a8",
   "metadata": {},
   "source": [
    "![Model Types](./images/model_types.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8392accc",
   "metadata": {},
   "source": [
    "**Multi-Scale Architecture**\n",
    "\n",
    "Disadvantage of normalizing flows is that they operate on the exact same dimensions as the input.\n",
    "    \n",
    "    If the input is high-dimensional, so is the latent space,  will requires larger computational cost to learn suitable transformations. \n",
    "    \n",
    "    However, in image domain, many pixels contain less information. \n",
    "\n",
    "Multi-scale architecture : After the first 𝑁  flow transformations, we split off half of the latent dimensions and directly evaluate them on the prior. The other half is run through  𝑁  more flow transformations, and depending on the size of the input, we split it again in half or stop overall at this position. \n",
    "\n",
    "    Squeeze and split:\n",
    "\n",
    "![Squeeze and Split](./images/squeeze_split.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35de283",
   "metadata": {},
   "source": [
    "**Structural Causal Model**\n",
    "\n",
    "(e_g, e_h, e_x) ~ N(0, 1)\n",
    "\n",
    "G ~ Bern(e_g)\n",
    "\n",
    "X ~ F(e_x, g, h)\n",
    "\n",
    "![SCM](./images/scm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94aa71f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "from pyro.nn import PyroModule, pyro_method\n",
    "from pyro.distributions import Normal, TransformedDistribution\n",
    "from pyro.distributions.torch_transform import ComposeTransformModule\n",
    "from pyro.distributions.conditional import ConditionalTransformedDistribution\n",
    "from pyro.distributions.transforms import (\n",
    "    Spline, ExpTransform, ComposeTransform, ConditionalAffineCoupling,\n",
    "    GeneralizedChannelPermute, SigmoidTransform\n",
    "    )\n",
    "\n",
    "from src.normalizingFlowsSCM.transforms import (\n",
    "    ReshapeTransform, SqueezeTransform,\n",
    "    TransposeTransform, LearnedAffineTransform,\n",
    "    ConditionalAffineTransform, ActNorm\n",
    "    )\n",
    "\n",
    "from pyro.nn import DenseNN\n",
    "from src.normalizingFlowsSCM.arch import BasicFlowConvNet\n",
    "\n",
    "\n",
    "class FlowSCM(pyroModule):\n",
    "    def __init__(self, use_affine_ex=True, **kwargs)\n",
    "        super.__init__(**kwargs)\n",
    "\n",
    "        self.num_scales = 2\n",
    "\n",
    "        self.register_buffer(\"glasses_base_loc\", torch.zeros([1, ], requires_grad=False))\n",
    "        self.register_buffer(\"glasses_base_scale\", torch.ones([1, ], requires_grad=False))\n",
    "\n",
    "        self.register_buffer(\"glasses_flow_lognorm_loc\", torch.zeros([], requires_grad=False))\n",
    "        self.register_buffer(\"glasses_flow_lognorm_scale\", torch.ones([], requires_grad=False))\n",
    "\n",
    "        self.glasses_flow_components = ComposeTransformModule([Spline(1)])\n",
    "        self.glasses_flow_constraint_transforms = ComposeTransform([self.glasses_flow_lognorm,\n",
    "            SigmoidTransform()])\n",
    "        self.glasses_flow_transforms = ComposeTransform([self.glasses_flow_components,\n",
    "            self.glasses_flow_constraint_transforms])\n",
    "\n",
    "        glasses_base_dist = Normal(self.glasses_base_loc, self.glasses_base_scale).to_event(1)\n",
    "        self.glasses_dist = TransformedDistribution(glasses_base_dist, self.glasses_flow_transforms)\n",
    "        glasses_ = pyro.sample(\"glasses_\", self.glasses_dist)\n",
    "        glasses = pyro.sample(\"glasses\", dist.Bernoulli(glasses_))\n",
    "        glasses_context = self.glasses_flow_constraint_transforms.inv(glasses_)\n",
    "\n",
    "        self.x_transforms = self._build_image_flow()\n",
    "        self.register_buffer(\"x_base_loc\", torch.zeros([1, 64, 64], requires_grad=False))\n",
    "        self.register_buffer(\"x_base_scale\", torch.ones([1, 64, 64], requires_grad=False))\n",
    "        x_base_dist = Normal(self.x_base_loc, self.x_base_scale).to_event(3)\n",
    "        cond_x_transforms = ComposeTransform(\n",
    "            ConditionalTransformedDistribution(x_base_dist, self.x_transforms)\n",
    "            .condition(context).transforms\n",
    "            ).inv\n",
    "        cond_x_dist = TransformedDistribution(x_base_dist, cond_x_transforms)\n",
    "\n",
    "        x = pyro.sample(\"x\", cond_x_dist)\n",
    "\n",
    "        return x, glasses\n",
    "\n",
    "\n",
    "    def _build_image_flow(self):\n",
    "        self.trans_modules = ComposeTransformModule([])\n",
    "        self.x_transforms = []\n",
    "        self.x_transforms += [self._get_preprocess_transforms()]\n",
    "\n",
    "        c = 1\n",
    "        for _ in range(self.num_scales):\n",
    "            self.x_transforms.append(SqueezeTransform())\n",
    "            c *= 4\n",
    "\n",
    "            for _ in range(self.flows_per_scale):\n",
    "                if self.use_actnorm:\n",
    "                    actnorm = ActNorm(c)\n",
    "                    self.trans_modules.append(actnorm)\n",
    "                    self.x_transforms.append(actnorm)\n",
    "\n",
    "                gcp = GeneralizedChannelPermute(channels=c)\n",
    "                self.trans_modules.append(gcp)\n",
    "                self.x_transforms.append(gcp)\n",
    "\n",
    "                self.x_transforms.append(TransposeTransform(torch.tensor((1, 2, 0))))\n",
    "\n",
    "                ac = ConditionalAffineCoupling(c // 2, BasicFlowConvNet(c // 2, self.hidden_channels, (c // 2, c // 2), 2))\n",
    "                self.trans_modules.append(ac)\n",
    "                self.x_transforms.append(ac)\n",
    "\n",
    "                self.x_transforms.append(TransposeTransform(torch.tensor((2, 0, 1))))\n",
    "\n",
    "            gcp = GeneralizedChannelPermute(channels=c)\n",
    "            self.trans_modules.append(gcp)\n",
    "            self.x_transforms.append(gcp)\n",
    "\n",
    "        self.x_transforms += [\n",
    "            ReshapeTransform((4**self.num_scales, 32 // 2**self.num_scales, 32 // 2**self.num_scales), (1, 32, 32))\n",
    "        ]\n",
    "\n",
    "        if self.use_affine_ex:\n",
    "            affine_net = DenseNN(2, [16, 16], param_dims=[1, 1])\n",
    "            affine_trans = ConditionalAffineTransform(context_nn=affine_net, event_dim=3)\n",
    "\n",
    "            self.trans_modules.append(affine_trans)\n",
    "            self.x_transforms.append(affine_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d322f0",
   "metadata": {},
   "source": [
    "**Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f67699d",
   "metadata": {},
   "source": [
    "**Bits Per Dimension (BPD)**\n",
    "\n",
    "As a final piece for calculating our loss function we use a concept called Bits Per Dimension (BPD)\n",
    "\n",
    "This loss calculates the number of bits needed to represent some sample x’ in our distribution P(X), with less bits corresponding to a larger likelihood\n",
    "\n",
    "We change the base of the log likelihood to base 2 and then divide by the product over the dimensions of our image (which is the width and height)\n",
    "\n",
    "Essentially we normalize over the dimensions we have over our images to allow for comparison between images of varying resolutions \n",
    "\n",
    "    This is important as we change the image resolutions to help speed up training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42db9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "008efcf1",
   "metadata": {},
   "source": [
    "**Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab4275a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8e68549",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "1. Pawlowski, N., Castro, D. C., & Glocker, B. (2020). Deep structural causal models for tractable counterfactual inference. arXiv preprint arXiv:2006.06485.\n",
    "2. Normalizing Flows - Introduction (Part 1) — Pyro Tutorials 1.7.0 documentation\n",
    "3. Cartoon Dataset \n",
    "4. Normalizing Flows for image modeling\n",
    "5. Dinh, L., Sohl-Dickstein, J., and Bengio, S. (2017). “Density estimation using Real NVP,” In: 5th International Conference on Learning Representations, ICLR 2017.\n",
    "6. Ho, J., Chen, X., Srinivas, A., Duan, Y., and Abbeel, P. (2019). “Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design,” in Proceedings of the 36th International Conference on Machine Learning, vol. 97, pp. 2722–2730\n",
    "7. Flow-based Deep Generative Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbc93ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
